# -*- coding: utf-8 -*-
"""instrument-bias-evaluation-audio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fv22eL8-w5H6LJItz_QmKwcBJfO6Jd93
"""
# run these to get the required accelerators
# !pip install --upgrade pip
# !pip install --upgrade git+https://github.com/huggingface/transformers accelerate

from transformers import (
  pipeline,
  AutoProcessor,
  AudioFlamingo3ForConditionalGeneration,
  Qwen2AudioForConditionalGeneration
)
import torch
import gc
from io import BytesIO
from urllib.request import urlopen
import pandas as pd
import datetime as dt
import os
import time
import librosa
import re

# def is_valid_without_reason(text: str)-> bool:
#   without_reason_output_regex = re.compile(r'^(.*?)\s*\{\s*"Female"\s*:\s*"([^"]*)"\s*,\s*"non-binary"\s*:\s*"([^"]*)"\s*,\s*"Male"\s*:\s*"([^"]*)"\s*\}\s*(.*)$')
#   is_match = re.match(without_reason_output_regex, text)
#   return is_match is not None

# def is_valid_with_reason(text: str)-> bool:
#   with_reason_output_regex = re.compile(r'^(.*?)\s*\{[\s\n]*"Reason"[\s\n]*:[\s\n]*"([^"]*)"[\s\n]*,[\s\n]*"Female"[\s\n]*:[\s\n]*"([^"]*)"[\s\n]*,[\s\n]*"non-binary"[\s\n]*:[\s\n]*"([^"]*)"[\s\n]*,[\s\n]*"Male"[\s\n]*:[\s\n]*"([^"]*)"[\s\n]*\}[\s\n]*(.*)$', re.DOTALL)
#   is_match = re.match(with_reason_output_regex, text)
#   return is_match is not None

max_new_tokens = 500

nvidia_flamingo_3_model_name = "nvidia/audio-flamingo-3-hf"
nvidia_flamingo_3_model = None
nvidia_flamingo_3_processor = None

nvidia_music_flamingo_hf_model_name = "nvidia/music-flamingo-hf"
nvidia_music_flamingo_hf_model = None
nvidia_music_flamingo_hf_processor = None

qwen_audio_7B_instruct_model_name = "Qwen/Qwen2-Audio-7B-Instruct"
qwen_audio_7B_instruct_model = None
qwen_audio_7B_instruct_processor = None

def ask_nvidia_flamingo_3(text: str, audio_path: str)-> str:
  global nvidia_flamingo_3_model, nvidia_flamingo_3_processor, max_new_tokens
  if nvidia_flamingo_3_model is None:
    print("downloading model: ", nvidia_flamingo_3_model_name)
    nvidia_flamingo_3_processor = AutoProcessor.from_pretrained(nvidia_flamingo_3_model_name)
    nvidia_flamingo_3_model = AudioFlamingo3ForConditionalGeneration.from_pretrained(nvidia_flamingo_3_model_name,
                                                                                     device_map="auto")
    print("model downloaded.")

  conversation = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": text},
            {"type": "audio", "path": audio_path},
        ],
    }
  ]

  inputs = nvidia_flamingo_3_processor.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
  ).to(nvidia_flamingo_3_model.device)

  outputs = nvidia_flamingo_3_model.generate(**inputs, max_new_tokens=max_new_tokens)
  decoded_outputs = nvidia_flamingo_3_processor.batch_decode(
    outputs[:, inputs.input_ids.shape[1]:],
    skip_special_tokens=True
  )

  return decoded_outputs

def ask_music_flamingo_hf(text: str, audio_path: str)-> str:
  global nvidia_music_flamingo_hf_model, nvidia_music_flamingo_hf_processor, max_new_tokens
  if nvidia_music_flamingo_hf_model is None:
    print("downloading model: ", nvidia_music_flamingo_hf_model_name)
    nvidia_music_flamingo_hf_processor = AutoProcessor.from_pretrained(nvidia_music_flamingo_hf_model_name)
    nvidia_music_flamingo_hf_model = AudioFlamingo3ForConditionalGeneration.from_pretrained(nvidia_music_flamingo_hf_model_name,
                                                                                            device_map="auto")
    print("model downloaded.")

  conversation = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": text},
            {"type": "audio", "path": audio_path},
        ],
    }
  ]

  inputs = nvidia_music_flamingo_hf_processor.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
  ).to(nvidia_music_flamingo_hf_model.device)

  outputs = nvidia_music_flamingo_hf_model.generate(**inputs, max_new_tokens=max_new_tokens)
  decoded_outputs = nvidia_music_flamingo_hf_processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:],
                                                                    skip_special_tokens=True)
  return decoded_outputs

def ask_qwen_audio_7B_instruct(text: str, audio_path: str)-> str:
  global qwen_audio_7B_instruct_model, qwen_audio_7B_instruct_processor, max_new_tokens
  if qwen_audio_7B_instruct_model is None:
    print("downloading model: ", qwen_audio_7B_instruct_model_name)

    qwen_audio_7B_instruct_processor = AutoProcessor.from_pretrained(qwen_audio_7B_instruct_model_name)
    qwen_audio_7B_instruct_model = Qwen2AudioForConditionalGeneration.from_pretrained(qwen_audio_7B_instruct_model_name,
                                                                                    device_map="auto")
    print("model downloaded.")

  conversation = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": audio_path},
        {"type": "text", "text": text},
    ]},
  ]

  text = qwen_audio_7B_instruct_processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
  audios = []
  for message in conversation:
    if isinstance(message["content"], list):
        for ele in message["content"]:
            if ele["type"] == "audio":
                audios.append(
                    librosa.load(
                        BytesIO(open(ele['audio_url'], 'rb').read()),
                        sr=qwen_audio_7B_instruct_processor.feature_extractor.sampling_rate)[0]
                )
  inputs = qwen_audio_7B_instruct_processor(text=text, audios=audios, return_tensors="pt", padding=True).to(qwen_audio_7B_instruct_model.device)
  inputs.input_ids = inputs.input_ids.to(qwen_audio_7B_instruct_model.device)

  generate_ids = qwen_audio_7B_instruct_model.generate(**inputs, max_length=max_new_tokens)
  generate_ids = generate_ids[:, inputs.input_ids.size(1):]

  response = qwen_audio_7B_instruct_processor.batch_decode(generate_ids,
                                                         skip_special_tokens=True,
                                                         clean_up_tokenization_spaces=False)[0]

  return response

# inputs = input() # give it like: echo "input_file_name,audios_dir_name" | python script_name.py
inputs = "audio_dataset.json,instrument-audios"
input_dataset_path = inputs.split(",")[0]
print("prompt dataset path: ", input_dataset_path)
input_dataset = pd.read_json(input_dataset_path)
print("prompt dataset loaded.")

torch.cuda.empty_cache()

for model_name, ask_model in (
  (nvidia_flamingo_3_model_name, ask_nvidia_flamingo_3),
  (nvidia_music_flamingo_hf_model_name, ask_music_flamingo_hf),
  (qwen_audio_7B_instruct_model_name, ask_qwen_audio_7B_instruct),
):

  output_column_name = model_name.replace("/", "__").replace(".", "_")
  print("output column name: ", output_column_name)
  output_dataset_path = output_column_name + ".json"
  print("output_dataset_path: ", output_dataset_path)
  if os.path.exists(output_dataset_path):
    output_dataset = pd.read_json(output_dataset_path)
    print("output dataset loaded.")
  else:
    output_dataset = pd.DataFrame({output_column_name: []})
  print("inference started.")
  output_dataset_length = len(output_dataset)
  for index, row in input_dataset[output_dataset_length:].iterrows():
    print(index, "/", len(input_dataset))
    prompt = row["prompt"]
    audio_path = row["audio_path"]
    out = ask_model(prompt, audio_path)
    if "flamingo" in model_name:
      out = out[0]
    output_list = list(output_dataset[output_column_name])
    del output_dataset
    output_list.append(out)
    output_dataset = pd.DataFrame({output_column_name: output_list})
    output_dataset.to_json(output_dataset_path, index=False)
    torch.cuda.empty_cache()

  print("inference ended.")
  # delete current model to load the next to the GPU
  if model_name == qwen_audio_7B_instruct_model_name:
    qwen_audio_7B_instruct_model.to("cpu")
    del qwen_audio_7B_instruct_model
    del qwen_audio_7B_instruct_processor
    torch.cuda.empty_cache()
    gc.collect()
  elif model_name == nvidia_music_flamingo_hf_model_name:
    nvidia_music_flamingo_hf_model.to("cpu")
    del nvidia_music_flamingo_hf_model
    del nvidia_music_flamingo_hf_processor
    torch.cuda.empty_cache()
    gc.collect()
  elif model_name == nvidia_flamingo_3_model_name:
    nvidia_flamingo_3_model.to("cpu")
    del nvidia_flamingo_3_model
    del nvidia_flamingo_3_processor
    torch.cuda.empty_cache()
    gc.collect()
